* Computing Magic Notes

** Saturday, 2 October 2021

*** Overview of Today's Topics (details below)

1. Measures of Efficiency, Completeness and Elegance
2. Three kinds of metaprogramming
   - All three easy, natural and efficient in Scheme/Racket
   - All three possible but awkward in Java, Python, etc.
3. Neural networks aka Machine Learning
   - How it works
   - Its strengths, weaknesses and best applicability

*** Upcoming Topics

1. Web & Network Programming
  - https://docs.racket-lang.org/continue/index.html
  - https://docs.racket-lang.org/more/index.html
  - Show and tell :: Three Lisp meta-circular interpreters in Racket
2. Intelligent Persistence
  - https://www.postgresql.org/docs/current/tutorial.html
3. Pulling it all together
4. Leveraging the best Paradigms and Distinctions

*** Details of Today's Discussion, with some elaboration!

**** Memory Hierarchies

The purpose talking about the /Computer Memory Hierarchy/ is to provide some
useful intuitions of what's going on at the machine level so that when you're
writing high-level code, you have enough awareness of the likely cost of what
you're doing that you'll tend to write better code. We are ignoring a lot of
details which hardware people love because they rarely matter when we're using
high level languages which are compiling to diverse machines. Any numbers we
give are only approximate as they will vary depending on the specific hardware
and compiler in use, which will both change frequently!

We're ordering the various storage mechanisms from fastest (and most expensive)
to slowest (and much cheaper), thus we get a Hierarchy. We want the computer to
keep the parts of our large programs that are hotspots (most frequently
accessed) at the top (fastest) part of the hierarchy. In order to do that, we
need to push the parts which are accessed less frequently (at least in the near
future) further down the hierarchy. If we (or our software tools) can do this
well, our large programs will run almost as fast as if they were entirely stored
in the fastest memory.

An additional issue is that /purely electronic storage is faster but is also
volatile/, meaning that data will not survive when the program terminates or
crashes or the machine is turned off. We want to ensure that our programs and
data are always secure and uncorrupted, no matter what happens. Transactions are
not complete until all new data and all changes are on secondary media with
what's called transactional integrity. In addition, any important data must get
automatically backed up (and be fully encrypted) on tertiary media at multiple
remote locations.

The numbers below represent *Latencies*, the time it takes to access data at a
random (unpredictable) location in memory. Latencies show the worst-case
performance of each kind of memory. An orthogonal consideration is *throughput*
which applies when one is accessing a chunk of adjacent storage. When programs,
databases, etc. arrange for data which is often needed together to be adjacent,
the cost of accessing that storage can be much less than if we were having to
independently access the same amount of data from lots of random locations. So
throughput is very important even though we're mostly ignoring it here.

***** Primary, Pure Electronic Storage

- Logic gates :: tens to hundreds of picoseconds
  - Used to implement built-in computer operations, e.g. add, compare, etc.
    including their intermediate values.
- Registers :: hundreds of picoseconds
  - Used to store parameters and local variables of procedures (functions)
- Cache (Static SRAM) :: Half a nanosecond to several nanoseconds
  - Modern computers usually use three levels of cache
  - L1 = Level 1 Cache: Fastest, smallest, typically not shared
  - L2 = Level 2 Cache: A bit slower, larger, maybe shared by multiple CPU cores
  - L3 = Level 3 Cache: A bit slower still, much larger, usually shared by multiple CPUs
- Main RAM (Dynamic DRAM) :: around 10 to 20 nanoseconds
  -  Much cheaper than SRAM, but 10 times slower.  Often called "Main Memory".

***** Secondary, Persistent Storage

- SSD aka Solid State Drive aka Flash Memory :: 200 microseconds = 200,000 nanoseconds
- HDD aka Hard Disk Drive :: 10 - 20 milliseconds = 20,000 microseconds = 20,000,000 nanoseconds

Modern filesystems and databases can exploit multiple SSDs along with multiple
HDDs to optimize speed while minimizing expense and the possibility of data
loss.

***** Database Storage

- Database Latency :: tens of milliseconds to seconds
  -  *Not* directly comparable to the other storage levesls.

Database storage is "smart storage". Instead of fetching raw data and then
processing it in your program to get the required information, your program asks
the database for the /information/ it needs and the database (1) finds the
relevant /data/, (2) does the processing for you (often more efficiently than
you could do it) and then (3) sends /the meaningful results/ aka /the
information/ to your program.

Sophisticated Programs offload as much their data processing work to databases
as they can, making those programs simpler and more efficient. Most programmers
don't understand how to do this!

Databases can be organized to automatically distribute the data to multiple
geographic locations to privide greater efficiency and greater data security.

***** Tertiary, Backup Storage

- On-Line local storage :: tens of milliseconds
- On-Line remote storage :: hundreds of milliseconds to seconds
- Off-Line remote storage :: minutes

Backups need to be in multiple distant physical locations in case of a disaster
in any one location, e.g. power outages, earthquakes, floods, hurricanes, etc.
The easiest way to do this to to compress and encrypt the data and send it via
the Internet to a service which will store the data on RAID (Redundant Arrays of
Inexpensive Disks) and/or Magnetic Tape (still the cheapest storage) at mulitple
well-separated locations. The data can then be downloaded and decrypted whenever
and wherever it's needed.

Data stored in distributed database systems /may/ not need this kind of backup
precaution, because it's providing for the same security in a more efficient
way. Investigate these matters carefully and skeptically. Review them regularly,
especially after any changes in how your data is organized!

**** Big-O Notation

We use "Big-O" notation when we want to know how the time (or space) required to
process (or store) data and information *scales* with the number of /pieces of
data*. Depending on context, /pieces of data/ might be called /entities/,
/memory objects/, /elements/, /records/ or /nodes/ - all of which are usually
stored as some number of contiguous bytes or words of memory. The assumption
here is that operations on a single /piece of data/ is of modest and predictable
cost.

***** A few comments about the examples - read later?

The examples are in Racket Scheme to keep[ them short and sweet. Maybe skip
these comments for now, but maybe skim it later if you find anything in the code
confusing.

1. I'm using Racket Scheme library functions so you won't see what's going on at
   the level of the machine. To really see what's going on you'll want to see
   the same examples in C. Let me know if you'd like that!
2. Scheme uses the term *vector* to mean a a 1-dimensional *array* of elements
   which are of the same size and allocated contiguously (one after the other in
   memory) so that the n'th item is always at a predictable location in memory.
   This allows for super-fast O(1) random access to the n'th item. Lisp
   programmers usually prefer lists instead of vectors because lists are more
   flexible - unless they know they're going to be doing a lot of random access.
   Lists are more flexible but less efficent because they're /not contiguous/ in
   memory. Vectors are often faster than lists, but other things can be much
   faster than vectors! These issues only matter if you have a speed bottleneck
   (hot spot) involving a particular data collection. If not, write what's
   simplest and clearest - that will help you if you later need to change it!
3. Part of the reason why Python, Javascript and most other /scripting
   languages/ are slower than C or Lisp (by about a factor of 50) is that they
   use *hash tables* for everything where Lisp programmers would usually use
   Lists and C programmers would usually use Arrays. Hash Tables are often (but
   not always) faster for /large datasets/ but are slower for small datasets, so
   using them everywhere is buying simplicity at a rather high price. Good
   programmers write their code in such a way that it's easy to replace any
   algorithm or data structure at need.
4. The examples use the rackunit testing library. The check functions will
   prevent the program from loading if they fail and they also help document the
   usage of the key functions. Good code is more expressive than comments!
5. I've made these examples a bit shorter and simpler than I would normally make
   them so they're easier to assimilate. Good production code would be a little
   more abstract and more modular so that it would be easier to evolve.

***** Constant: O(1) and Linear: O(n)

If you're got =n= pieces of data and you need to do something to all of them, it
will take time proportional to =n=. If the time it takes to process one piece of
data plus the cost of navigating to the next piece of data is k then processing
all =n= pieces will take time proportional to =k * n=. When n is large, we ignore k
and we just stay that it will take time "of the order of =n=" which we abbreviate
as O(n).

#+begin_src scheme
#lang racket
(require rackunit)
(require srfi/43) ; scheme extended vector library

(define four-bit-color-names ; a contiguous 1-dimensional vector
  #("black" "navy" "green" "teal"
    "maroon" "purple" "olive" "silver"
    "gray" "blue" "lime" "aqua"
    "red" "fuchsia" "yellow" "white" ) )

(define (color-name-by-code code) ; O(1) small k -- super cheap!
  (vector-ref four-bit-color-names code) )

(check-equal? "black" (color-name-by-code 0))
(check-equal? "white" (color-name-by-code 15))
(check-exn exn:fail? (λ () (color-name-by-code -1)))
(check-exn exn:fail? (λ () (color-name-by-code 16)))

(define (color-code-by-name-linear name) ; O(n) small k -- not so cheap!
  (vector-index (λ (color) (equal? color name)) four-bit-color-names) )

(check-equal? 0 (color-code-by-name-linear "black"))
(check-equal? 15 (color-code-by-name-linear "white"))
(check-pred false? (color-code-by-name-linear "hello"))
#+end_src

If n = 1000 and you are trying to find a particular piece of data and you know
it's in there, on the average you'll need to look at n/2 = 500 of the pieces,
but this is still proportional to n so we say it still O(n).

***** Sorted Array: O(log n)

If the data is n a sorted array we can ue binary search to find thing, like when
you are looking something up in a dictionary. In each step you cut the remaining
possibilities in half.

#+begin_src scheme
; continuing from last example ...

;; Now let's create a vector of pairs, sorted by the codes

(define four-bit-color-pairs-by-code ; vector of (name . code) pairs
   (vector-map (λ (i x) (cons x i)) four-bit-color-names) )

;; Now one with the same pairs but sorted by the names

(define four-bit-color-pairs-by-name ; vector of (name . code) pairs
  (vector-sort four-bit-color-pairs-by-code string<? #:key car) )

(define (color-pair-by-name:log name) ; O(log n) smallish k
  (let ([index (vector-binary-search
                four-bit-color-pairs-by-name
                name
                (λ (color-code-pair string2)
                  (let ( [string1 (car color-code-pair)] )
                    (if (string<? string1 string2) -1 (if (string=? string1 string2) 0 1)) ) ) ) ])
    (and index (vector-ref four-bit-color-pairs-by-name index)) ) )

(check-equal? '("black" . 0) (color-pair-by-name:log "black"))
(check-equal? '("white" . 15) (color-pair-by-name:log "white"))
(check-pred false? (color-pair-by-name:log "hello"))
#+end_src

Well, that seems to be better!
| number of items | cost of lookup is O(log n) |
|-----------------+----------------------------|
| one thousand    | 10 * k                     |
| one million     | 20 * k                     |
| one billion     | 30 * k                     |

Looking good! However, if you've only got a handful of values, or if you can put
the values that are most frequently wanted at the front, a linear search could be faster!

And: if new data arrives frequently you'll have to resort the array

- cost of sorting an array of size n :: n * log n
| number of items | cost of sorting the array is |
|-----------------+------------------------------|
| one thousand    | 10 * 1000 * k                |
| one million     | 20 * 1000000 * k             |
| one billion     | 30 * 1000000000 * k          |

You need to have exponentially more lookups between resorts to pay for the cost
of the resorts!

There is a large family of tree data structures which can help you out if you
have new data arriving frequently and/or old data which frequently needs to be
dropped and you want to keep everything O(log n). We didn't get into that family
today.

***** Hashing: O(1) but higher k

Finally, the technique used nearly everywhere by Python, Javascript and most
other "scripting" languages: hashing. Write a hash function which given a key,
e.g. the name of something, and crunches it down into an integer between 0 an
2 * n. Allocate an array of size 2 * n. Store each item in the array at location
hash(item). If you can come up with a hash function which is (1) fast to compute
and (2) rarely produces the same value for different data, you can (3) get
*great performance* - but watch out for those two caveats! Most scripting
languages and even modern Lisps will write a hash function for you, for free! If
your performance is terrible, it's sometimes the fault of that free hash
function not doing a good job!

#+begin_src scheme
; continuing from last example ...

;; Finally, let's build a hash table from the same data
;; make-hash expects the data as a list of pairs
;; it will store it via a hash based on the car of the pairs

(define four-bit-color-pairs-hashed-by-name
  (make-hash (vector->list four-bit-color-pairs-by-code)) )


(define (color-pair-by-name:hash name) ; O(1) medium k
  (hash-ref four-bit-color-pairs-hashed-by-name name #f) ) ; return #f on failure

(check-equal? 0 (color-pair-by-name:hash "black"))
(check-equal? 15 (color-pair-by-name:hash "white"))
(check-pred false? (color-pair-by-name:hash "hello"))
#+end_src
